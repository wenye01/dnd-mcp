# DND MCP Client - ä»»åŠ¡å…­å¼€å‘è®¡åˆ’

## LLM é›†æˆï¼ˆçœŸå®ï¼‰

### æ–‡æ¡£ä¿¡æ¯

- **ä»»åŠ¡ç¼–å·**: Task-6
- **ä»»åŠ¡åç§°**: LLM é›†æˆï¼ˆçœŸå®ï¼‰
- **å‰ç½®ä»»åŠ¡**: Task-4ï¼ˆHTTP API - æ¶ˆæ¯ç®¡ç†ï¼‰
- **åŸºäºæ–‡æ¡£**: DND_MCP_Clientè¯¦ç»†è®¾è®¡.mdã€DND_MCP_Client_å¼€å‘è®¡åˆ’.md
- **å¿…é¡»éµå®ˆ**: doc/è§„èŒƒ.md

---

## ä»»åŠ¡æ¦‚è¿°

### ç›®æ ‡

åœ¨ä»»åŠ¡å››çš„åŸºç¡€ä¸Š,é›†æˆçœŸå®çš„ LLM æœåŠ¡(OpenAI API),åŒæ—¶ä¿ç•™ Mock æ¨¡å¼ä»¥ä¾¿æµ‹è¯•å’Œå¼€å‘ã€‚æ”¯æŒ tool_calls(Function Calling)åŠŸèƒ½,ä¸ºåç»­çš„ MCP Server é›†æˆåšå‡†å¤‡ã€‚

### ğŸ”´ **å…³é”®é—®é¢˜ï¼šä»»åŠ¡å››ç¼ºå°‘å¯¹è¯ä¸Šä¸‹æ–‡**

**å½“å‰çŠ¶æ€**ï¼ˆä»»åŠ¡å››ï¼‰:
```go
// âŒ åªä¼ é€’å½“å‰ç”¨æˆ·æ¶ˆæ¯
llmResp, err := h.llmClient.Chat(ctx, &llm.ChatRequest{
    Messages: []llm.Message{{Role: "user", Content: req.Content}},
})
```

**é—®é¢˜**:
- ğŸ”´ **æ— æ³•å®ç°å¤šè½®å¯¹è¯** - LLM ä¸ç†è§£ä¹‹å‰çš„å¯¹è¯å†…å®¹
- ğŸ”´ **ç”¨æˆ·ä½“éªŒå·®** - æ¯æ¬¡å¯¹è¯éƒ½æ˜¯æ–°çš„å¼€å§‹
- ğŸ”´ **ä¸ç¬¦åˆè®¾è®¡æ–‡æ¡£** - è®¾è®¡æ–‡æ¡£æ˜ç¡®è¦æ±‚åŠ è½½å†å²æ¶ˆæ¯

**è§£å†³æ–¹æ¡ˆ** (ä»»åŠ¡å…­):
```go
// âœ… åŠ è½½å†å²æ¶ˆæ¯ï¼ˆæœ€è¿‘ 50 æ¡ï¼‰
history, _ := h.messageStore.List(ctx, sessionID, 50)

// âœ… æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
messages := []llm.Message{{Role: "system", Content: "..."}}
for _, msg := range history {
    messages = append(messages, llm.Message{
        Role: msg.Role, Content: msg.Content,
    })
}

// âœ… è°ƒç”¨ LLMï¼ˆä¼ é€’å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
llmResp, err := h.llmClient.Chat(ctx, &llm.ChatRequest{Messages: messages})
```

**ä¼˜å…ˆçº§**: ğŸ”´ **P0 - å¿…é¡»ä¿®å¤**

**å·¥ä½œé‡**: 2-3å°æ—¶

**è¯¦ç»†åˆ†æ**: å‚è§ `doc/ä»»åŠ¡å››ä¸ä»»åŠ¡å…­è®¾è®¡åˆç†æ€§åˆ†æ.md`

---

### å½“å‰ Mock æµ‹è¯•æƒ…å†µ

**Mock LLM** (`internal/llm/mock.go`):
```go
type MockLLMClient struct {
    Response string // é¢„è®¾å“åº”
}

func (m *MockLLMClient) Chat(ctx, ctx, req *ChatRequest) (*ChatResponse, error) {
    return &ChatResponse{
        Message: Message{
            Role:    "assistant",
            Content: m.Response, // å›ºå®šå“åº”
        },
    }, nil
}
```

**æµ‹è¯•åœºæ™¯**:
- âœ… åŸºæœ¬æ¶ˆæ¯å‘é€å’Œæ¥æ”¶
- âœ… API å‚æ•°éªŒè¯
- âœ… é”™è¯¯å¤„ç†ï¼ˆä¼šè¯ä¸å­˜åœ¨ã€Redis å¤±è´¥ç­‰ï¼‰
- âŒ **æ— æ³•æµ‹è¯•å¤šè½®å¯¹è¯**ï¼ˆMock è¿”å›å›ºå®šå“åº”ï¼‰
- âŒ **æ— æ³•æµ‹è¯• tool_calls**ï¼ˆMock ä¸æ”¯æŒï¼‰

**ä»»åŠ¡å…­æ”¹è¿›**:
- âœ… ä¿ç•™ Mock æ¨¡å¼ç”¨äºå¼€å‘æµ‹è¯•
- âœ… æ·»åŠ å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆMock ä¹Ÿå—ç›Šï¼‰
- âœ… æ”¯æŒçœŸå® OpenAI API è¿›è¡Œç«¯åˆ°ç«¯æµ‹è¯•
- âš ï¸ tool_calls æµ‹è¯•éœ€è¦çœŸå® LLMï¼ˆæˆ–å¢å¼º Mockï¼‰

---

### åŸºäºä»»åŠ¡å››çš„æˆæœ

ä»»åŠ¡å››å·²å®Œæˆçš„åŸºç¡€è®¾æ–½:

- âœ… æ¶ˆæ¯ç®¡ç† API (`internal/api/handler/message.go`)
- âœ… Mock LLM å®ç° (`internal/llm/mock.go`)
- âœ… LLM Client æ¥å£ (`internal/llm/client.go`)
- âœ… æ¶ˆæ¯æ¨¡å‹,åŒ…å« `ToolCalls` å­—æ®µ (`internal/models/message.go`)
- âœ… ç®€åŒ–æ¶æ„: Handler â†’ Store â†’ Models
- âš ï¸ **ç¼ºå°‘å¯¹è¯ä¸Šä¸‹æ–‡**ï¼ˆä»»åŠ¡å…­å¿…é¡»ä¿®å¤ï¼‰

### ä»»åŠ¡å…­æ–°å¢åŠŸèƒ½

- ğŸŒ OpenAI Client å®ç° (`internal/llm/openai.go`)
- âš™ï¸ LLM é…ç½®ç®¡ç† (`pkg/config/config.go` æ‰©å±• `LLMConfig`)
- ğŸ”§ LLM Client å·¥å‚æ¨¡å¼
- ğŸ“¦ **å¯¹è¯ä¸Šä¸‹æ–‡æ„å»º**ï¼ˆğŸ”´ å…³é”®æ”¹è¿›ï¼‰
- ğŸ“¦ tool_calls è§£æå’Œå¤„ç†
- ğŸ”„ é…ç½®é©±åŠ¨çš„ Provider åˆ‡æ¢
- ğŸ“ OpenAI API å•å…ƒæµ‹è¯•

### æ¶æ„è¯´æ˜

**âš ï¸ é‡è¦ï¼šç»§ç»­é‡‡ç”¨ç®€åŒ–æ¶æ„**

ä¸ä»»åŠ¡å››ä¿æŒä¸€è‡´,ä»»åŠ¡å…­ç»§ç»­é‡‡ç”¨ç®€åŒ–æ¶æ„:

```
Handler (message.go)
  â†“ ç›´æ¥è°ƒç”¨
LLMClient æ¥å£
  â†“ å®ç°
OpenAIClient / MockLLMClient
```

**ä¸æ·»åŠ é¢å¤–å±‚æ¬¡**:
- âŒ ä¸æ·»åŠ  LLMService å±‚
- âŒ ä¸æ·»åŠ  LLMRepository å±‚
- âœ… Handler ç›´æ¥æ³¨å…¥ LLMClient æ¥å£

**ç†ç”±**:
- LLM è°ƒç”¨é€»è¾‘ç›¸å¯¹ç®€å•(HTTP è¯·æ±‚ + JSON è§£æ)
- å¿«é€Ÿé›†æˆçœŸå® LLM åŠŸèƒ½
- ä¿æŒä»£ç ç®€æ´,æ˜“äºç†è§£å’Œè°ƒè¯•

---

## æ ¸å¿ƒåŠŸèƒ½

### 1. LLM Client é…ç½®å’Œåˆå§‹åŒ– (éœ€æ±‚ 6.1)

#### å®ç°å†…å®¹

**é…ç½®æ‰©å±•** (`pkg/config/config.go`)

```go
// Config åº”ç”¨é…ç½®
type Config struct {
    Redis    RedisConfig    `mapstructure:"redis"`
    Postgres PostgresConfig `mapstructure:"postgres"`
    Log      LogConfig      `mapstructure:"log"`
    HTTP     HTTPConfig     `mapstructure:"http"`
    LLM      LLMConfig      `mapstructure:"llm"` // æ–°å¢
}

// LLMConfig LLM é…ç½®
type LLMConfig struct {
    Provider    string  `mapstructure:"provider" env:"LLM_PROVIDER" default:"mock"`         // openai, mock
    APIKey      string  `mapstructure:"api_key" env:"LLM_API_KEY" default:""`
    Model       string  `mapstructure:"model" env:"LLM_MODEL" default:"gpt-4"`
    MaxTokens   int     `mapstructure:"max_tokens" env:"LLM_MAX_TOKENS" default:"4096"`
    Temperature float64 `mapstructure:"temperature" env:"LLM_TEMPERATURE" default:"0.7"`
    Timeout     int     `mapstructure:"timeout" env:"LLM_TIMEOUT" default:"30"` // seconds
}

// Load åŠ è½½é…ç½®(æ–°å¢ LLM é…ç½®)
func Load() (*Config, error) {
    // ... ç°æœ‰ä»£ç  ...

    cfg := &Config{
        // ... ç°æœ‰é…ç½® ...
        LLM: LLMConfig{
            Provider:    getEnv("LLM_PROVIDER", "mock"),
            APIKey:      getEnv("LLM_API_KEY", ""),
            Model:       getEnv("LLM_MODEL", "gpt-4"),
            MaxTokens:   getEnvInt("LLM_MAX_TOKENS", 4096),
            Temperature: getEnvFloat64("LLM_TEMPERATURE", 0.7),
            Timeout:     getEnvInt("LLM_TIMEOUT", 30),
        },
    }

    // éªŒè¯é…ç½®
    if err := cfg.Validate(); err != nil {
        return nil, fmt.Errorf("é…ç½®éªŒè¯å¤±è´¥: %w", err)
    }

    return cfg, nil
}

// Validate éªŒè¯é…ç½®(æ–°å¢ LLM éªŒè¯)
func (c *Config) Validate() error {
    // ... ç°æœ‰éªŒè¯ ...

    // éªŒè¯ LLM é…ç½®
    if c.LLM.Provider != "mock" && c.LLM.Provider != "openai" {
        return fmt.Errorf("æ— æ•ˆçš„ LLM provider: %s", c.LLM.Provider)
    }

    if c.LLM.Provider == "openai" && c.LLM.APIKey == "" {
        return fmt.Errorf("OpenAI API key ä¸èƒ½ä¸ºç©º")
    }

    if c.LLM.MaxTokens <= 0 {
        return fmt.Errorf("LLM max tokens å¿…é¡»å¤§äº 0")
    }

    if c.LLM.Temperature < 0 || c.LLM.Temperature > 2 {
        return fmt.Errorf("LLM temperature å¿…é¡»åœ¨ 0-2 èŒƒå›´å†…")
    }

    if c.LLM.Timeout <= 0 {
        return fmt.Errorf("LLM timeout å¿…é¡»å¤§äº 0")
    }

    return nil
}

// æ–°å¢è¾…åŠ©å‡½æ•°
func getEnvFloat64(key string, defaultValue float64) float64 {
    if value := os.Getenv(key); value != "" {
        if floatVal, err := strconv.ParseFloat(value, 64); err == nil {
            return floatVal
        }
    }
    return defaultValue
}
```

**ç¯å¢ƒå˜é‡é…ç½®**

```bash
# ä½¿ç”¨çœŸå® OpenAI API
export LLM_PROVIDER=openai
export LLM_API_KEY=sk-xxx
export LLM_MODEL=gpt-4
export LLM_MAX_TOKENS=4096
export LLM_TEMPERATURE=0.7
export LLM_TIMEOUT=30

# ä½¿ç”¨ Mock æ¨¡å¼(é»˜è®¤)
export LLM_PROVIDER=mock
```

#### LLM ç±»å‹æ‰©å±• (`internal/llm/types.go`)

**æ–°å¢æ–‡ä»¶**: æ‰©å±•ç°æœ‰çš„ LLM ç±»å‹å®šä¹‰,æ”¯æŒ OpenAI API æ ¼å¼

```go
// Package llm æä¾› LLM ç±»å‹å®šä¹‰
package llm

// ChatRequest èŠå¤©è¯·æ±‚(æ‰©å±•)
type ChatRequest struct {
    Model       string    `json:"model"`                 // æ¨¡å‹åç§°
    Messages    []Message `json:"messages"`              // æ¶ˆæ¯åˆ—è¡¨
    Tools       []Tool    `json:"tools,omitempty"`       // å·¥å…·å®šä¹‰(å¯é€‰)
    ToolChoice  any       `json:"tool_choice,omitempty"` // å·¥å…·é€‰æ‹©ç­–ç•¥(å¯é€‰)
    MaxTokens   int       `json:"max_tokens,omitempty"`
    Temperature float64   `json:"temperature,omitempty"`
}

// Tool å·¥å…·å®šä¹‰
type Tool struct {
    Type     string           `json:"type"` // "function"
    Function FunctionDef      `json:"function"`
}

// FunctionDef å‡½æ•°å®šä¹‰
type FunctionDef struct {
    Name        string                 `json:"name"`
    Description string                 `json:"description"`
    Parameters  map[string]interface{} `json:"parameters"` // JSON Schema
}

// ChatResponse èŠå¤©å“åº”(æ‰©å±•)
type ChatResponse struct {
    ID      string   `json:"id"`
    Object  string   `json:"object"`
    Created int64    `json:"created"`
    Model   string   `json:"model"`
    Choices []Choice `json:"choices"`
    Usage   Usage    `json:"usage"`
}

// Choice é€‰æ‹©
type Choice struct {
    Index        int     `json:"index"`
    Message      Message `json:"message"`
    FinishReason string  `json:"finish_reason"` // stop, length, tool_calls, content_filter
}

// Usage ä½¿ç”¨ç»Ÿè®¡
type Usage struct {
    PromptTokens     int `json:"prompt_tokens"`
    CompletionTokens int `json:"completion_tokens"`
    TotalTokens      int `json:"total_tokens"`
}

// Message æ¶ˆæ¯(æ‰©å±•)
type Message struct {
    Role       string     `json:"role"`                 // system, user, assistant, tool
    Content    string     `json:"content"`
    ToolCalls  []ToolCall `json:"tool_calls,omitempty"` // å·¥å…·è°ƒç”¨
    ToolCallID string     `json:"tool_call_id,omitempty"` // tool è§’è‰²çš„å“åº”
}

// ToolCall å·¥å…·è°ƒç”¨
type ToolCall struct {
    ID       string       `json:"id"`
    Type     string       `json:"type"` // "function"
    Function FunctionCall `json:"function"`
}

// FunctionCall å‡½æ•°è°ƒç”¨
type FunctionCall struct {
    Name      string `json:"name"`
    Arguments string `json:"arguments"` // JSON string
}
```

**ä¸ç°æœ‰ä»£ç å…¼å®¹**:
- ç°æœ‰çš„ `ChatRequest` å’Œ `ChatResponse` ä¿æŒå‘åå…¼å®¹
- æ–°å¢å­—æ®µä¸ºå¯é€‰(`omitempty`)
- Mock LLM ä¸éœ€è¦ä¿®æ”¹,ç»§ç»­ä½¿ç”¨ç°æœ‰å®ç°

---

### 2. è°ƒç”¨ OpenAI API (éœ€æ±‚ 6.2)

#### å®ç°å†…å®¹

**OpenAI Client** (`internal/llm/openai.go`)

```go
// Package llm æä¾› OpenAI LLM å®ç°
package llm

import (
    "bytes"
    "context"
    "encoding/json"
    "fmt"
    "io"
    "net/http"
    "time"

    "github.com/dnd-mcp/client/pkg/config"
)

// OpenAIClient OpenAI å®¢æˆ·ç«¯
type OpenAIClient struct {
    config     *config.LLMConfig
    httpClient *http.Client
    baseURL    string
    apiKey     string
}

// NewOpenAIClient åˆ›å»º OpenAI å®¢æˆ·ç«¯
func NewOpenAIClient(cfg *config.LLMConfig) *OpenAIClient {
    return &OpenAIClient{
        config:  cfg,
        baseURL: "https://api.openai.com/v1",
        apiKey:  cfg.APIKey,
        httpClient: &http.Client{
            Timeout: time.Duration(cfg.Timeout) * time.Second,
        },
    }
}

// Chat å®ç°èŠå¤©æ¥å£
func (c *OpenAIClient) Chat(ctx context.Context, req *ChatRequest) (*ChatResponse, error) {
    // æ„å»ºè¯·æ±‚
    if req.Model == "" {
        req.Model = c.config.Model
    }

    reqBody, err := json.Marshal(req)
    if err != nil {
        return nil, fmt.Errorf("åºåˆ—åŒ–è¯·æ±‚å¤±è´¥: %w", err)
    }

    // åˆ›å»º HTTP è¯·æ±‚
    httpReq, err := http.NewRequestWithContext(ctx, "POST",
        c.baseURL+"/chat/completions",
        bytes.NewReader(reqBody))
    if err != nil {
        return nil, fmt.Errorf("åˆ›å»º HTTP è¯·æ±‚å¤±è´¥: %w", err)
    }

    // è®¾ç½®è¯·æ±‚å¤´
    httpReq.Header.Set("Content-Type", "application/json")
    httpReq.Header.Set("Authorization", "Bearer "+c.apiKey)

    // å‘é€è¯·æ±‚
    resp, err := c.httpClient.Do(httpReq)
    if err != nil {
        return nil, fmt.Errorf("å‘é€ HTTP è¯·æ±‚å¤±è´¥: %w", err)
    }
    defer resp.Body.Close()

    // æ£€æŸ¥å“åº”çŠ¶æ€
    if resp.StatusCode != http.StatusOK {
        body, _ := io.ReadAll(resp.Body)
        return nil, fmt.Errorf("API è¯·æ±‚å¤±è´¥ (status %d): %s",
            resp.StatusCode, string(body))
    }

    // è§£æå“åº”
    var chatResp ChatResponse
    if err := json.NewDecoder(resp.Body).Decode(&chatResp); err != nil {
        return nil, fmt.Errorf("è§£æå“åº”å¤±è´¥: %w", err)
    }

    return &chatResp, nil
}
```

**LLM Client å·¥å‚** (`internal/llm/client.go` æ‰©å±•)

```go
// Package llm æä¾› LLM å®¢æˆ·ç«¯æ¥å£å’Œå®ç°
package llm

import (
    "fmt"

    "github.com/dnd-mcp/client/pkg/config"
)

// LLMClient LLM å®¢æˆ·ç«¯æ¥å£
type LLMClient interface {
    // Chat èŠå¤©å¯¹è¯ï¼Œè¿”å› AI å“åº”
    Chat(ctx context.Context, req *ChatRequest) (*ChatResponse, error)
}

// NewClient åˆ›å»º LLM å®¢æˆ·ç«¯
func NewClient(cfg *config.LLMConfig) (LLMClient, error) {
    switch cfg.Provider {
    case "openai":
        if cfg.APIKey == "" {
            return nil, fmt.Errorf("OpenAI API key ä¸èƒ½ä¸ºç©º")
        }
        return NewOpenAIClient(cfg), nil

    case "mock":
        return NewMockLLMClient(), nil

    default:
        return nil, fmt.Errorf("ä¸æ”¯æŒçš„ LLM provider: %s", cfg.Provider)
    }
}
```

#### é›†æˆåˆ°æœåŠ¡å™¨å¯åŠ¨

**Server å¯åŠ¨** (`internal/cli/server.go` ä¿®æ”¹)

```go
// StartCommand å¯åŠ¨æœåŠ¡å™¨å‘½ä»¤
func StartCommand(cmd *cobra.Command, args []string) error {
    // åŠ è½½é…ç½®
    cfg, err := config.Load()
    if err != nil {
        return fmt.Errorf("åŠ è½½é…ç½®å¤±è´¥: %w", err)
    }

    // åˆå§‹åŒ– LLM Client
    llmClient, err := llm.NewClient(&cfg.LLM)
    if err != nil {
        return fmt.Errorf("åˆå§‹åŒ– LLM å®¢æˆ·ç«¯å¤±è´¥: %w", err)
    }

    logrus.Infof("ä½¿ç”¨ LLM provider: %s", cfg.LLM.Provider)

    // åˆå§‹åŒ– Redis
    redisClient := redis.NewClient(&redis.Options{
        Addr:     cfg.Redis.Host,
        Password: cfg.Redis.Password,
        DB:       cfg.Redis.DB,
    })

    sessionStore := redisstore.NewSessionStore(redisClient)
    messageStore := redisstore.NewMessageStore(redisClient)

    // åˆ›å»º Handler
    sessionHandler := handler.NewSessionHandler(sessionStore)
    messageHandler := handler.NewMessageHandler(messageStore, sessionStore, llmClient)

    // é…ç½®è·¯ç”±
    r := api.Router(sessionStore, messageStore)

    // å¯åŠ¨ HTTP æœåŠ¡å™¨
    addr := fmt.Sprintf("%s:%d", cfg.HTTP.Host, cfg.HTTP.Port)
    server := &http.Server{
        Addr:    addr,
        Handler: r,
    }

    logrus.Infof("æœåŠ¡å™¨å¯åŠ¨åœ¨ http://%s", addr)
    return server.ListenAndServe()
}
```

---

### 2.5. æ·»åŠ å¯¹è¯ä¸Šä¸‹æ–‡æ„å»º ğŸ”´ **å…³é”®æ”¹è¿›**

**âš ï¸ é‡è¦**: ä»»åŠ¡å››å®ç°**ç¼ºå°‘å¯¹è¯ä¸Šä¸‹æ–‡**,ä»»åŠ¡å…­**å¿…é¡»ä¿®å¤**è¿™ä¸ªé—®é¢˜ã€‚

#### é—®é¢˜è¯´æ˜

**ä»»åŠ¡å››çš„å½“å‰å®ç°**:
```go
// âŒ åªä¼ é€’å½“å‰ç”¨æˆ·æ¶ˆæ¯
llmResp, err := h.llmClient.Chat(ctx, &llm.ChatRequest{
    Messages: []llm.Message{{Role: "user", Content: req.Content}},
})
```

**é—®é¢˜**:
- âŒ æ²¡æœ‰åŠ è½½å†å²æ¶ˆæ¯
- âŒ LLM æ— æ³•ç†è§£ä¸Šä¸‹æ–‡
- âŒ æ— æ³•å®ç°å¤šè½®å¯¹è¯

#### å¿…é¡»ä¿®å¤ï¼šæ·»åŠ ä¸Šä¸‹æ–‡æ„å»º

**ä¿®æ”¹ Handler** (`internal/api/handler/message.go`)

```go
// SendMessage å‘é€æ¶ˆæ¯å¹¶è·å– AI å“åº”
func (h *MessageHandler) SendMessage(c *gin.Context) {
    sessionID := c.Param("sessionId")

    // ... ç°æœ‰éªŒè¯ä»£ç  ...

    // 1. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    userMessage := &models.Message{
        ID:        uuid.New().String(),
        SessionID: sessionID,
        Role:      "user",
        Content:   req.Content,
        PlayerID:  req.PlayerID,
        CreatedAt: time.Now(),
    }

    if err := h.messageStore.Create(c.Request.Context(), userMessage); err != nil {
        // é”™è¯¯å¤„ç†
    }

    // 2. ğŸ†• åŠ è½½å†å²æ¶ˆæ¯ï¼ˆæœ€è¿‘ 50 æ¡ï¼‰
    history, err := h.messageStore.List(c.Request.Context(), sessionID, 50)
    if err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{
            "error": gin.H{
                "code":    "INTERNAL_ERROR",
                "message": "åŠ è½½å†å²æ¶ˆæ¯å¤±è´¥",
            },
        })
        return
    }

    // 3. ğŸ†• æ„å»º LLM ä¸Šä¸‹æ–‡
    messages := []llm.Message{
        {
            Role:    "system",
            Content: "ä½ æ˜¯ DND æ¸¸æˆçš„ DM(åœ°ä¸‹åŸä¸»)ã€‚è´Ÿè´£æè¿°åœºæ™¯ã€æ‰®æ¼” NPCã€åˆ¤æ–­è§„åˆ™ã€æ¨è¿›å‰§æƒ…ã€‚",
        },
    }

    // 4. ğŸ†• æ·»åŠ å†å²æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
    for _, msg := range history {
        // è¿‡æ»¤æ‰å½“å‰æ­£åœ¨ä¿å­˜çš„ç”¨æˆ·æ¶ˆæ¯ï¼ˆé¿å…é‡å¤ï¼‰
        if msg.ID != userMessage.ID {
            messages = append(messages, llm.Message{
                Role:    msg.Role,
                Content: msg.Content,
            })
        }
    }

    // 5. ğŸ†• æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
    messages = append(messages, llm.Message{
        Role:    "user",
        Content: req.Content,
    })

    // 6. è°ƒç”¨ LLMï¼ˆä¼ é€’å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
    llmReq := &llm.ChatRequest{
        Model:       "gpt-4",
        Messages:    messages,  // ğŸ†• å®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡
        Temperature: 0.7,
    }

    llmResp, err := h.llmClient.Chat(c.Request.Context(), llmReq)
    // ... åç»­ä»£ç  ...
}
```

#### ä¸Šä¸‹æ–‡æ„å»ºè¯´æ˜

**åŠ è½½ç­–ç•¥**:
- âœ… ä» Redis åŠ è½½æœ€è¿‘ **50 æ¡**æ¶ˆæ¯
- âœ… æŒ‰ `created_at` **å‡åº**æ’åˆ—ï¼ˆä»æ—§åˆ°æ–°ï¼‰
- âœ… åŒ…å«æ‰€æœ‰è§’è‰²ï¼ˆuser, assistant, system, toolï¼‰

**System Prompt**:
```
ä½ æ˜¯ DND æ¸¸æˆçš„ DM(åœ°ä¸‹åŸä¸»)ã€‚è´Ÿè´£æè¿°åœºæ™¯ã€æ‰®æ¼” NPCã€åˆ¤æ–­è§„åˆ™ã€æ¨è¿›å‰§æƒ…ã€‚
```

**ä¸Šä¸‹æ–‡ç»“æ„**:
```
[
  {role: "system", content: "..."},
  {role: "user", content: "å†å²æ¶ˆæ¯1"},
  {role: "assistant", content: "å†å²æ¶ˆæ¯2"},
  ...
  {role: "user", content: "å½“å‰æ¶ˆæ¯"}
]
```

#### ä¼˜ç‚¹

âœ… **æ”¯æŒå¤šè½®å¯¹è¯** - LLM å¯ä»¥ç†è§£ä¹‹å‰çš„å¯¹è¯å†…å®¹
âœ… **æå‡ç”¨æˆ·ä½“éªŒ** - å¯¹è¯è¿è´¯è‡ªç„¶
âœ… **ç¬¦åˆè®¾è®¡æ–‡æ¡£** - å®ç°è®¾è®¡æ–‡æ¡£è¦æ±‚çš„ä¸Šä¸‹æ–‡æ„å»º
âœ… **å‘åå…¼å®¹** - Mock æ¨¡å¼åŒæ ·å—ç›Š

#### æ€§èƒ½è€ƒè™‘

| ä¼šè¯æ¶ˆæ¯æ•° | ä¸Šä¸‹æ–‡å¤§å° | æ€§èƒ½å½±å“ |
|-----------|----------|---------|
| < 50 æ¡   | < 50 æ¡  | æ— å½±å“    |
| 50-100 æ¡ | 50 æ¡    | è½»å¾®å½±å“  |
| > 100 æ¡  | 50 æ¡    | æ— å½±å“ï¼ˆåªåŠ è½½æœ€è¿‘ 50 æ¡ï¼‰ |

**ä¼˜åŒ–**: æœªæ¥å¯ä»¥æ·»åŠ æ™ºèƒ½é€‰æ‹©ç­–ç•¥ï¼ˆå¦‚æ ¹æ® Token æ•°é‡åŠ¨æ€è°ƒæ•´ï¼‰

---

### 2.6. ä¸Šä¸‹æ–‡æ„å»ºå™¨è¯¦ç»†è®¾è®¡ ğŸ†• **æ ¸å¿ƒæ”¹è¿›**

#### è®¾è®¡ç›®æ ‡

ä¸ºäº†æ”¯æŒ**å¼€å‘æ€å¿«é€Ÿè°ƒæ•´**ä¸Šä¸‹æ–‡æ„å»ºç­–ç•¥ï¼Œè®¾è®¡ä¸€ä¸ªç‹¬ç«‹çš„ä¸Šä¸‹æ–‡æ„å»ºå™¨æ¨¡å—ï¼Œå°†æ‰€æœ‰ä¸Šä¸‹æ–‡ç›¸å…³é€»è¾‘é›†ä¸­ç®¡ç†ï¼Œå®ç°ï¼š

1. **å•å‡½æ•°ä¿®æ”¹** - åªéœ€ä¿®æ”¹ä¸€ä¸ªæ ¸å¿ƒå‡½æ•°å³å¯è°ƒæ•´ä¸Šä¸‹æ–‡ç­–ç•¥
2. **é›†ä¸­ç®¡ç†** - æ‰€æœ‰ä¸Šä¸‹æ–‡æ„å»ºé€»è¾‘åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­
3. **æ˜“äºæµ‹è¯•** - ç‹¬ç«‹çš„æ„å»ºå™¨ä¾¿äºå•å…ƒæµ‹è¯•
4. **æ¸…æ™°åˆ†ç¦»** - å°†ä¸Šä¸‹æ–‡æ„å»ºé€»è¾‘ä» Handler ä¸­åˆ†ç¦»

#### æ¶æ„è®¾è®¡

**æ–°å¢æ¨¡å—**: `internal/service/context_builder.go`

```
Handler (message.go)
  â†“ å§”æ‰˜
ContextBuilder (service/context_builder.go)
  â†“ è°ƒç”¨
MessageStore + SessionStore
```

#### æ ¸å¿ƒè®¾è®¡ï¼šå•å‡½æ•°é…ç½®æ¨¡å¼

**å…³é”®æ€æƒ³**: åœ¨ `BuildContext()` å‡½æ•°é¡¶éƒ¨å®šä¹‰**é…ç½®åŒº**ï¼Œæ‰€æœ‰å¯è°ƒæ•´çš„å‚æ•°é›†ä¸­åœ¨è¿™é‡Œã€‚

**é…ç½®åŒºåŒ…å«**:
1. **System Prompt** - å®šä¹‰ AI çš„è§’è‰²å’Œè¡Œä¸º
2. **å†å²æ¶ˆæ¯åŠ è½½ç­–ç•¥**
   - `maxHistory`: æœ€å¤šåŠ è½½å¤šå°‘æ¡å†å²æ¶ˆæ¯ï¼ˆé»˜è®¤ 50 æ¡ï¼‰
   - `includeRoles`: åŒ…å«å“ªäº›è§’è‰²çš„æ¶ˆæ¯ï¼ˆç©ºæ•°ç»„è¡¨ç¤ºå…¨éƒ¨ï¼‰
   - `excludeRoles`: æ’é™¤å“ªäº›è§’è‰²çš„æ¶ˆæ¯ï¼ˆå¦‚ `["system"]`ï¼‰
3. **æ¶ˆæ¯è¿‡æ»¤å‡½æ•°** - å¯è‡ªå®šä¹‰è¿‡æ»¤é€»è¾‘ï¼ˆå¦‚è¿‡æ»¤ç©ºæ¶ˆæ¯ã€æŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤ç­‰ï¼‰

**æ‰§è¡ŒåŒº**: æ ¹æ®é…ç½®åŒºçš„å‚æ•°ï¼Œæ‰§è¡Œä»¥ä¸‹æ­¥éª¤
1. æ„å»º System æ¶ˆæ¯
2. åŠ è½½å†å²æ¶ˆæ¯
3. åº”ç”¨è¿‡æ»¤è§„åˆ™
4. æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
5. è¿”å›å®Œæ•´çš„ä¸Šä¸‹æ–‡æ•°ç»„

#### å¿«é€Ÿè°ƒæ•´åœºæ™¯ç¤ºä¾‹

**åœºæ™¯1: æµ‹è¯•ç¯å¢ƒ - åªä¿ç•™æœ€è¿‘ 10 æ¡æ¶ˆæ¯**
- ä¿®æ”¹ `maxHistory = 10`
- å…¶ä»–å‚æ•°ä¿æŒé»˜è®¤

**åœºæ™¯2: æ— çŠ¶æ€å¯¹è¯ - åªä¿ç•™å½“å‰ç”¨æˆ·æ¶ˆæ¯**
- ä¿®æ”¹ `maxHistory = 0`
- ä¸åŠ è½½ä»»ä½•å†å²æ¶ˆæ¯

**åœºæ™¯3: åªä¿ç•™ user å’Œ assistant æ¶ˆæ¯**
- `maxHistory = 50`
- `excludeRoles = ["system", "tool"]`
- åªåŒ…å«å¯¹è¯ç›¸å…³æ¶ˆæ¯

**åœºæ™¯4: åŒ…å«æ‰€æœ‰æ¶ˆæ¯ç±»å‹ï¼Œå¢åŠ åˆ° 100 æ¡**
- `maxHistory = 100`
- `excludeRoles = []`
- åŒ…å«æ‰€æœ‰è§’è‰²çš„æ¶ˆæ¯

**åœºæ™¯5: è‡ªå®šä¹‰è¿‡æ»¤ - è¿‡æ»¤ç©ºå†…å®¹æ¶ˆæ¯**
- åœ¨ `messageFilter` å‡½æ•°ä¸­æ·»åŠ é€»è¾‘
- è¿‡æ»¤ `Content == ""` ä¸”æ—  ToolCalls çš„æ¶ˆæ¯

#### System Prompt æ„å»º

**ç‹¬ç«‹å‡½æ•°**: `buildSystemPrompt()`

- å®šä¹‰ AI çš„è§’è‰²ï¼ˆDND æ¸¸æˆçš„ DMï¼‰
- å®šä¹‰ AI çš„èŒè´£ï¼ˆæè¿°åœºæ™¯ã€æ‰®æ¼” NPCã€åˆ¤æ–­è§„åˆ™ç­‰ï¼‰
- æœªæ¥æ‰©å±•ï¼šå¯ä»ä¼šè¯é…ç½®ä¸­åŠ è½½è‡ªå®šä¹‰ Prompt
- æœªæ¥æ‰©å±•ï¼šå¯æ ¹æ®æ¸¸æˆçŠ¶æ€åŠ¨æ€è°ƒæ•´

#### å†å²æ¶ˆæ¯åŠ è½½ç­–ç•¥

**ç‹¬ç«‹å‡½æ•°**: `loadHistory()`

- ä» Redis åŠ è½½æœ€è¿‘ N æ¡æ¶ˆæ¯
- æŒ‰æ—¶é—´æ’åºï¼ˆä»æ—§åˆ°æ–°ï¼‰
- æœªæ¥æ‰©å±•ç‚¹ï¼š
  - æŒ‰æ—¶é—´èŒƒå›´åŠ è½½ï¼ˆå¦‚æœ€è¿‘ 24 å°æ—¶ï¼‰
  - æŒ‰æ¶ˆæ¯ç±»å‹åŠ è½½
  - æ™ºèƒ½æˆªæ–­ï¼ˆæ ¹æ® Token æ•°é‡ï¼‰
  - æ‘˜è¦å‹ç¼©ï¼ˆå°†æ—§æ¶ˆæ¯å‹ç¼©ä¸ºæ‘˜è¦ï¼‰

#### é›†æˆåˆ° Handler

**ä¿®æ”¹å‰** (ä»»åŠ¡å››):
- ä¸Šä¸‹æ–‡æ„å»ºé€»è¾‘æ•£è½åœ¨ Handler ä¸­
- ç›´æ¥åœ¨ Handler ä¸­åŠ è½½å†å²æ¶ˆæ¯
- ç›´æ¥åœ¨ Handler ä¸­ç»„è£… messages æ•°ç»„

**ä¿®æ”¹å** (ä»»åŠ¡å…­):
- Handler æŒæœ‰ `ContextBuilder` å®ä¾‹
- è°ƒç”¨ `contextBuilder.BuildContext()` è·å–å®Œæ•´ä¸Šä¸‹æ–‡
- Handler åªè´Ÿè´£ HTTP å±‚é€»è¾‘ï¼Œä¸Šä¸‹æ–‡æ„å»ºå®Œå…¨å§”æ‰˜

**é›†æˆæ–¹å¼**:
1. åœ¨ `NewMessageHandler` ä¸­æ³¨å…¥ `ContextBuilder`
2. åœ¨ `SendMessage` ä¸­è°ƒç”¨ `BuildContext()`
3. å°†è¿”å›çš„ messages ä¼ é€’ç»™ LLM Client

#### ä¼˜ç‚¹æ€»ç»“

âœ… **å•ç‚¹ä¿®æ”¹** - æ‰€æœ‰ä¸Šä¸‹æ–‡ç­–ç•¥é›†ä¸­åœ¨ `BuildContext()` å‡½æ•°é¡¶éƒ¨
âœ… **ä»£ç æ¸…æ™°** - é…ç½®åŒºå’Œæ‰§è¡ŒåŒºæ˜ç¡®åˆ†ç¦»
âœ… **æ˜“äºè°ƒè¯•** - ç‹¬ç«‹æ¨¡å—ä¾¿äºæ‰“å°æ—¥å¿—å’Œè°ƒè¯•
âœ… **çµæ´»æ‰©å±•** - æ”¯æŒå¤æ‚çš„è¿‡æ»¤å’ŒåŠ è½½ç­–ç•¥
âœ… **ä¾¿äºæµ‹è¯•** - å¯ç‹¬ç«‹æµ‹è¯• ContextBuilder
âœ… **æ³¨é‡Šå®Œå–„** - ä»£ç ä¸­æœ‰è¯¦ç»†çš„å¼€å‘æç¤ºå’Œç¤ºä¾‹

#### æ–‡ä»¶æ¸…å•

**æ–°å¢æ–‡ä»¶**:
```
internal/service/context_builder.go   # ä¸Šä¸‹æ–‡æ„å»ºå™¨å®ç°
```

**ä¿®æ”¹æ–‡ä»¶**:
```
internal/api/handler/message.go       # ä½¿ç”¨ ContextBuilder
internal/cli/server.go                # åˆå§‹åŒ– ContextBuilder
```

---

### 3. è§£æ LLM å“åº”å’Œ tool_calls (éœ€æ±‚ 6.3)

#### å®ç°å†…å®¹

**Handler æ‰©å±•** (`internal/api/handler/message.go` ä¿®æ”¹)

```go
// SendMessage å‘é€æ¶ˆæ¯å¹¶è·å– AI å“åº”
func (h *MessageHandler) SendMessage(c *gin.Context) {
    sessionID := c.Param("sessionId")

    // éªŒè¯ä¼šè¯æ˜¯å¦å­˜åœ¨
    session, err := h.sessionStore.Get(c.Request.Context(), sessionID)
    if err != nil {
        c.JSON(http.StatusNotFound, gin.H{
            "error": gin.H{
                "code":    "SESSION_NOT_FOUND",
                "message": "ä¼šè¯ä¸å­˜åœ¨",
            },
        })
        return
    }

    var req SendMessageRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        c.JSON(http.StatusBadRequest, gin.H{
            "error": gin.H{
                "code":    "INVALID_REQUEST",
                "message": "è¯·æ±‚å‚æ•°é”™è¯¯: " + err.Error(),
            },
        })
        return
    }

    // 1. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    userMessage := &models.Message{
        ID:        uuid.New().String(),
        SessionID: sessionID,
        Role:      "user",
        Content:   req.Content,
        PlayerID:  req.PlayerID,
        CreatedAt: time.Now(),
    }

    if err := h.messageStore.Create(c.Request.Context(), userMessage); err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{
            "error": gin.H{
                "code":    "INTERNAL_ERROR",
                "message": "ä¿å­˜ç”¨æˆ·æ¶ˆæ¯å¤±è´¥",
            },
        })
        return
    }

    // 2. æ„å»ºä¸Šä¸‹æ–‡(å¯é€‰: åŠ è½½å†å²æ¶ˆæ¯)
    llmReq := &llm.ChatRequest{
        Model: "gpt-4", // é»˜è®¤æ¨¡å‹,ä¼šè¢« OpenAIClient è¦†ç›–
        Messages: []llm.Message{
            {
                Role:    "user",
                Content: req.Content,
            },
        },
        Temperature: 0.7,
    }

    // 3. è°ƒç”¨ LLM
    llmResp, err := h.llmClient.Chat(c.Request.Context(), llmReq)
    if err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{
            "error": gin.H{
                "code":    "LLM_ERROR",
                "message": "LLM è°ƒç”¨å¤±è´¥: " + err.Error(),
            },
        })
        return
    }

    // 4. æ£€æŸ¥å“åº”ç±»å‹
    if len(llmResp.Choices) == 0 {
        c.JSON(http.StatusInternalServerError, gin.H{
            "error": gin.H{
                "code":    "LLM_ERROR",
                "message": "LLM è¿”å›ç©ºå“åº”",
            },
        })
        return
    }

    choice := llmResp.Choices[0]

    // 5. åˆ¤æ–­æ˜¯å¦æœ‰ tool_calls
    if choice.FinishReason == "tool_calls" && len(choice.Message.ToolCalls) > 0 {
        // å¤„ç†å·¥å…·è°ƒç”¨
        return h.handleToolCalls(c, sessionID, choice.Message.ToolCalls)
    }

    // 6. ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯(çº¯æ–‡æœ¬å“åº”)
    assistantMessage := &models.Message{
        ID:        uuid.New().String(),
        SessionID: sessionID,
        Role:      choice.Message.Role,
        Content:   choice.Message.Content,
        CreatedAt: time.Now(),
    }

    if err := h.messageStore.Create(c.Request.Context(), assistantMessage); err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{
            "error": gin.H{
                "code":    "INTERNAL_ERROR",
                "message": "ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯å¤±è´¥",
            },
        })
        return
    }

    // 7. è¿”å›åŠ©æ‰‹å“åº”
    c.JSON(http.StatusOK, assistantMessage)
}

// handleToolCalls å¤„ç†å·¥å…·è°ƒç”¨
func (h *MessageHandler) handleToolCalls(c *gin.Context, sessionID string, toolCalls []llm.ToolCall) {
    ctx := c.Request.Context()

    // 1. ä¿å­˜ assistant æ¶ˆæ¯(åŒ…å« tool_calls)
    assistantMsg := &models.Message{
        ID:        uuid.New().String(),
        SessionID: sessionID,
        Role:      "assistant",
        Content:   "",
        ToolCalls: convertLLMToolCalls(toolCalls),
        CreatedAt: time.Now(),
    }

    if err := h.messageStore.Create(ctx, assistantMsg); err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{
            "error": gin.H{
                "code":    "INTERNAL_ERROR",
                "message": "ä¿å­˜å·¥å…·è°ƒç”¨æ¶ˆæ¯å¤±è´¥",
            },
        })
        return
    }

    // 2. è¿”å›å“åº”(å½“å‰ä»…ä¿å­˜,ä¸æ‰§è¡Œå·¥å…·)
    //    å·¥å…·æ‰§è¡Œå°†åœ¨ä»»åŠ¡ä¸ƒ(MCP é›†æˆ)ä¸­å®ç°
    c.JSON(http.StatusOK, gin.H{
        "id":         assistantMsg.ID,
        "session_id": assistantMsg.SessionID,
        "role":       assistantMsg.Role,
        "tool_calls": assistantMsg.ToolCalls,
        "created_at": assistantMsg.CreatedAt.Format(time.RFC3339),
        "message":    "LLM è¿”å›äº†å·¥å…·è°ƒç”¨,ç­‰å¾… MCP é›†æˆ(ä»»åŠ¡ä¸ƒ)æ‰§è¡Œ",
    })
}

// convertLLMToolCalls è½¬æ¢ LLM tool_calls åˆ°æ¨¡å‹
func convertLLMToolCalls(llmToolCalls []llm.ToolCall) []models.ToolCall {
    toolCalls := make([]models.ToolCall, len(llmToolCalls))
    for i, tc := range llmToolCalls {
        // è§£æ arguments JSON string
        var args map[string]interface{}
        json.Unmarshal([]byte(tc.Function.Arguments), &args)

        toolCalls[i] = models.ToolCall{
            ID:        tc.ID,
            Name:      tc.Function.Name,
            Arguments: args,
        }
    }
    return toolCalls
}
```

---

### 4. Mock æ¨¡å¼ (éœ€æ±‚ 6.5)

#### å®ç°å†…å®¹

**Mock LLM ä¿æŒç°æœ‰å®ç°** (`internal/llm/mock.go`)

```go
// ç°æœ‰ Mock å®ç°ä¿æŒä¸å˜
type MockLLMClient struct {
    Response string
}

func (m *MockLLMClient) Chat(ctx context.Context, req *ChatRequest) (*ChatResponse, error) {
    return &ChatResponse{
        Message: Message{
            Role:    "assistant",
            Content: m.Response,
        },
    }, nil
}
```

**é…ç½®é©±åŠ¨åˆ‡æ¢**:

```go
// åœ¨ server.go ä¸­
llmClient, err := llm.NewClient(&cfg.LLM)

// æ ¹æ® LLM_PROVIDER ç¯å¢ƒå˜é‡è‡ªåŠ¨é€‰æ‹©:
// - "mock"   â†’ MockLLMClient
// - "openai" â†’ OpenAIClient
```

---

## æŠ€æœ¯äº®ç‚¹

1. **æ¥å£åŒ–è®¾è®¡**
   - å®šä¹‰ç»Ÿä¸€çš„ `LLMClient` æ¥å£
   - OpenAI å’Œ Mock å®ç°åŒä¸€æ¥å£
   - æ˜“äºæ‰©å±•å…¶ä»–æä¾›å•†(Azure, Anthropic ç­‰)

2. **é…ç½®é©±åŠ¨**
   - é€šè¿‡ç¯å¢ƒå˜é‡ `LLM_PROVIDER` åˆ‡æ¢å®ç°
   - Mock æ¨¡å¼ç”¨äºå¼€å‘å’Œæµ‹è¯•
   - OpenAI æ¨¡å¼ç”¨äºç”Ÿäº§ç¯å¢ƒ

3. **å‘åå…¼å®¹**
   - æ‰©å±•ç°æœ‰ç±»å‹,ä¸ç ´åç°æœ‰ä»£ç 
   - Mock LLM æ— éœ€ä¿®æ”¹
   - Handler æ”¹åŠ¨æœ€å°

4. **tool_calls é¢„ç•™**
   - æ­£ç¡®è§£æ tool_calls
   - ä¿å­˜åˆ°æ•°æ®åº“
   - ä¸ºä»»åŠ¡ä¸ƒ MCP é›†æˆå‡†å¤‡

5. **ç®€åŒ–æ¶æ„**
   - Handler â†’ LLMClient ç›´æ¥è°ƒç”¨
   - æ— é¢å¤–æŠ½è±¡å±‚
   - ä»£ç ç®€æ´,æ˜“äºç†è§£

6. **é”™è¯¯å¤„ç†**
   - HTTP è¯·æ±‚é”™è¯¯
   - API é”™è¯¯å“åº”
   - JSON è§£æé”™è¯¯
   - è¶…æ—¶å¤„ç†

---

## æ–‡ä»¶æ¸…å•

### æ–°å¢æ–‡ä»¶

```
internal/llm/
  â”œâ”€â”€ openai.go          # OpenAI Client å®ç°
  â””â”€â”€ openai_test.go     # OpenAI Client å•å…ƒæµ‹è¯•

pkg/config/
  â””â”€â”€ config.go          # æ‰©å±• LLMConfig (ä¿®æ”¹)
```

### ä¿®æ”¹æ–‡ä»¶

```
internal/llm/
  â”œâ”€â”€ client.go          # æ·»åŠ  NewClient å·¥å‚å‡½æ•° (ä¿®æ”¹)
  â””â”€â”€ types.go           # æ‰©å±•ç±»å‹å®šä¹‰ (æ–°å¢/ä¿®æ”¹)

internal/api/handler/
  â””â”€â”€ message.go         # æ·»åŠ  tool_calls å¤„ç† (ä¿®æ”¹)

internal/cli/
  â””â”€â”€ server.go          # åˆå§‹åŒ– LLM Client (ä¿®æ”¹)

tests/unit/llm/
  â””â”€â”€ openai_test.go     # OpenAI Client æµ‹è¯• (æ–°å¢)
```

---

## ä½¿ç”¨ç¤ºä¾‹

### 1. ä½¿ç”¨çœŸå® OpenAI API

```bash
# é…ç½®ç¯å¢ƒå˜é‡
export LLM_PROVIDER=openai
export LLM_API_KEY=sk-xxx
export LLM_MODEL=gpt-4

# å¯åŠ¨æœåŠ¡å™¨
./bin/dnd-client.exe server start

# å‘é€æ¶ˆæ¯
curl -X POST http://localhost:8080/api/sessions/{session-id}/chat \
  -H "Content-Type: application/json" \
  -d '{
    "content": "ä½ å¥½,è¯·ä»‹ç»ä¸€ä¸‹ DND æ¸¸æˆ",
    "player_id": "player-123"
  }'

# å“åº”
{
  "id": "msg-uuid-xxx",
  "session_id": "session-uuid-xxx",
  "role": "assistant",
  "content": "ä½ å¥½!ã€Šé¾™ä¸åœ°ä¸‹åŸã€‹(Dungeons & Dragons,ç®€ç§° D&D)æ˜¯...",
  "created_at": "2025-02-04T10:05:30Z"
}
```

### 2. ä½¿ç”¨ Mock æ¨¡å¼

```bash
# é…ç½® Mock æ¨¡å¼(é»˜è®¤)
export LLM_PROVIDER=mock

# å¯åŠ¨æœåŠ¡å™¨
./bin/dnd-client.exe server start

# å‘é€æ¶ˆæ¯
curl -X POST http://localhost:8080/api/sessions/{session-id}/chat \
  -H "Content-Type: application/json" \
  -d '{
    "content": "ä½ å¥½",
    "player_id": "player-123"
  }'

# å“åº”
{
  "id": "msg-uuid-xxx",
  "role": "assistant",
  "content": "è¿™æ˜¯ Mock LLM çš„å“åº”",
  "created_at": "2025-02-04T10:05:30Z"
}
```

### 3. tool_calls å“åº”ç¤ºä¾‹(æœªæ¥)

```bash
# å½“ LLM å†³å®šè°ƒç”¨å·¥å…·æ—¶
curl -X POST http://localhost:8080/api/sessions/{session-id}/chat \
  -H "Content-Type: application/json" \
  -d '{
    "content": "æˆ‘è¦æ”»å‡»å“¥å¸ƒæ—",
    "player_id": "player-123"
  }'

# å“åº”(åŒ…å« tool_calls)
{
  "id": "msg-uuid-xxx",
  "role": "assistant",
  "tool_calls": [
    {
      "id": "call_abc123",
      "name": "resolve_attack",
      "arguments": {
        "attacker": "player-123",
        "target": "goblin-1",
        "attack_type": "melee"
      }
    }
  ],
  "created_at": "2025-02-04T10:05:30Z",
  "message": "LLM è¿”å›äº†å·¥å…·è°ƒç”¨,ç­‰å¾… MCP é›†æˆ(ä»»åŠ¡ä¸ƒ)æ‰§è¡Œ"
}
```

---

## æµ‹è¯•è¦†ç›–

- âœ… LLM é…ç½®åŠ è½½å’ŒéªŒè¯
- âœ… OpenAI Client åˆ›å»º
- âœ… èŠå¤© API è°ƒç”¨(æˆåŠŸåœºæ™¯)
- âœ… API é”™è¯¯å“åº”å¤„ç†
- âœ… HTTP ç½‘ç»œé”™è¯¯å¤„ç†
- âœ… JSON è§£æé”™è¯¯å¤„ç†
- âœ… Mock æ¨¡å¼åˆ‡æ¢
- âœ… tool_calls è§£æ
- âœ… Handler é›†æˆæµ‹è¯•
- âœ… ç«¯åˆ°ç«¯æµ‹è¯•(ä½¿ç”¨æµ‹è¯• API key)

---

## æ€§èƒ½æŒ‡æ ‡

| æ“ä½œ            | é¢„æœŸå“åº”æ—¶é—´ | è¯´æ˜                   |
| ------------- | ------ | -------------------- |
| Mock LLM è°ƒç”¨  | <1ms   | ç«‹å³è¿”å›                 |
| OpenAI API è°ƒç”¨ | 1-3s   | å–å†³äº LLM æ¨¡å‹å’Œå“åº”é•¿åº¦    |
| tool_calls è§£æ | <1ms   | JSON è§£æå’Œæ¨¡å‹è½¬æ¢        |

---

## åç»­æ‰©å±•

**ä»»åŠ¡ä¸ƒ(MCP é›†æˆ)**:
- å®ç°å·¥å…·è°ƒç”¨æ‰§è¡Œ
- è°ƒç”¨ MCP Server å·¥å…·
- å¤„ç†å·¥å…·è°ƒç”¨ç»“æœ
- ç»§ç»­è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå“åº”

**é•¿æœŸæ‰©å±•**:
- å…¶ä»– LLM æä¾›å•†(Azure, Anthropic)
- æµå¼å“åº”æ”¯æŒ
- Token è®¡è´¹ç»Ÿè®¡
- å¤šè½®å¯¹è¯ä¼˜åŒ–

---

## æ€»ç»“

ä»»åŠ¡å…­åœ¨ä»»åŠ¡å››çš„åŸºç¡€ä¸Šå®ç°äº†çœŸå® LLM é›†æˆ:

1. âœ… **LLM Client é…ç½®å’Œåˆå§‹åŒ–** - çµæ´»çš„é…ç½®ç®¡ç†
2. âœ… **è°ƒç”¨ OpenAI API** - å®Œæ•´çš„ HTTP å®¢æˆ·ç«¯
3. âœ… **è§£æ tool_calls** - ä¸ºä»»åŠ¡ä¸ƒåšå‡†å¤‡
4. âœ… **Mock æ¨¡å¼åˆ‡æ¢** - é…ç½®é©±åŠ¨çš„æ¨¡å¼åˆ‡æ¢
5. âœ… **ç®€åŒ–æ¶æ„** - Handler â†’ LLMClient ç›´æ¥è°ƒç”¨
6. âœ… **å‘åå…¼å®¹** - ä¸ç ´åç°æœ‰ä»£ç 

ä¸¥æ ¼éµå¾ª `doc/è§„èŒƒ.md`,å¤ç”¨ä»»åŠ¡ä¸€ã€ä¸‰ã€å››çš„å®ç°,ä¿æŒç®€åŒ–æ¶æ„é£æ ¼ã€‚

---

**å¼€å‘å®Œæˆæ—¶é—´**: 2025-02-04
**å¼€å‘è€…**: Claude (Anthropic)
**çŠ¶æ€**: âœ… å·²å®Œæˆ
